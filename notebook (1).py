# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FK1hudmFg_Sd0pOAS33ZH_Hh9rYQtVG9

#Data Collection

Importing Neccesary Packages
"""

# Importing neccesary libraries
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

from imblearn.over_sampling import RandomOverSampler

from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import f1_score
from sklearn.ensemble import GradientBoostingClassifier

"""Loading Data"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/history.csv")

"""#Exploratory Data Analysis

Examining Basic Properties
"""

df.head(3)

df.info()

df.shape

"""Histogram of Unique Variable Counts Across the Features"""

unique_count = df.nunique()
unique_count

# Plotting all values except customer ID

df_filtered = df.drop(columns=['id'])

unique_counts = df_filtered.nunique()

plt.figure(figsize=(20, 6))
plt.bar(unique_counts.index, unique_counts.values, color='skyblue')
plt.xlabel('Variables')
plt.ylabel('Number of Unique Values')
plt.title('Histogram of Unique Variable Counts')
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Number of Customers"""

cell_count = df["id"].count()
print(cell_count)

"""Sum of Churned Customers"""

churn_count = df[df['label'] == 'Churn']['label'].count()
  print(churn_count)

"""Visualizing the Distribution of Target Variable"""

churn_counts = df['label'].value_counts()

# Plot a bar chart of churn vs. no churn
plt.figure(figsize=(6, 4))  # Adjust figure size if needed
plt.bar(churn_counts.index, churn_counts.values, color=['red', 'green'])  # Use red for churn, green for no churn
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Count of Churn vs. No Churn')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Churn Rate"""

print(churn_count/cell_count)

"""Assessing Intercorrelation Between Numeric Variables"""

corr = df.corr()

# Plot correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
plt.imshow(corr, cmap='coolwarm', interpolation='nearest')
plt.colorbar(label='Correlation')
plt.title('Correlation Matrix')
plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.columns)), corr.columns)
plt.tight_layout()
plt.show()

"""Assessing Missigness of Data"""

# Plotting histogram to visualize "missingness" distribution accross the features
missing_values_count = df.isnull().sum()

plt.figure(figsize=(15, 6))
plt.bar(missing_values_count.index, missing_values_count.values, color='skyblue')
plt.xlabel('Features')
plt.ylabel('Number of Missing Values')
plt.title('Histogram of Missing Values in Each Column')
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""#Data Pre-Processing

###Feature Engineering Part 1

**Handling Missing Values**
> We set a threshold to remove features exceeding a specific percentage of missing values, denoted as n. This threshold is determined based on the observed distribution of missing data within our dataset.
"""

def missing_threshold(df, threshold):
    missing_percentage = (df.isnull().sum() / len(df)) * 100 # Calculating % of missing values for each column
    columns_to_remove = missing_percentage[missing_percentage > threshold].index # Identifying columns exceeding the threshold
    df = df.drop(columns=columns_to_remove) # Removing them from dataframe
    return df

df = missing_threshold(df, 30) # Setting 30% missigness as our threshold

"""**Date-time Feature Extraction**
>Splitting date-time features into year and month was chosen for its ability to provide granular insights into temporal patterns. From examining basic data properties, we noticed date/time columns were not identified as date-time objects - the code below converts date/time columns into date-time object data type, and then performs feature extraction.
"""

def date_time_extraction(df):
  date_columns = []
  for col in df.columns:
      if ('_date' in col.lower() and 'use' not in col.lower()) or 'time' in col.lower() or '_at' in col.lower():
          date_columns.append(col)

  for col in date_columns:

      df[col] = pd.to_datetime(df[col]) # Converting column to datetime object

      df[col + '_month'] = df[col].dt.month # Extracting day and month
      df[col + '_year'] = df[col].dt.year

      # Drop original datetime column
      df.drop(columns=[col], inplace=True)

  return df

date_time_extraction(df)

"""*Prior to proceeding to subsequent feature engineering techniques, we will perform missing value imputation as part of our preprocessing steps*

###Missing Value Imputation

**KNN Imputation for date-time objects**
> We opted for KNN (K-Nearest Neighbours) imputation for date-time objects due to the absence of clear central tendencies in our dataset's time columns, making other methods like mean and mode imputation less suitable. KNN's predictive power captures intricate temporal patterns, ensuring more accurate imputation in line with the data's complexity.
"""

def impute_datetime(df):
  date_columns = []
  for col in df.columns:
        if ('_year' in col.lower() or '_month' in col.lower()):
            date_columns.append(col)

  for col in date_columns:
      imputer = KNNImputer(n_neighbors=5)
      df[[col]] = imputer.fit_transform(df[[col]])

  return df

df = impute_datetime(df)

"""**Median/Mode Imputation for categorical and numerical features**
> In light of our dataset's now 30% missingness, non-normal distribution, and predominance of categorical features, we utilized mode imputation for categorical features and median imputation for numerical features. This choice reflects the robustness of median imputation to outliers and its ability to maintain the data's central tendency, and mode imputation's preservation of original category frequencies.
"""

def feature_imputation(df):
  data_types = df.dtypes
  for c in df.columns:
    if data_types[c] == 'object':
              df[c].fillna(df[c].mode().iloc[0], inplace=True) # Mode value imputation for discrete values
    else:
              df[c].fillna(df[c].median(), inplace=True) # Median value imputation for continuous values
  return df

df = feature_imputation(df)

"""**Oversampling the minority class**
>Our exploratory data analysis highlighted the imbalance in the distribution of classes within our target feature. To mitigate the bias introduced by skewed class proportions, we chose to oversample the minority class (no churn) to ensure fair representation of both classes (churn and no churn).
"""

X = df.drop(columns=['label'])
y = df['label']

oversampler = RandomOverSampler(sampling_strategy='minority', random_state=42) # Initializing the RandomOverSampler

X_resampled, y_resampled = oversampler.fit_resample(X, y)


df_resampled = pd.DataFrame(X_resampled, columns=X.columns) # Converting the resampled data back to DataFrame
df_resampled['label'] = y_resampled

print(df_resampled['label'].value_counts()) # Checking the class distribution after oversampling

"""###Feature Engineering Part 2

**Feature Selection**
> We conducted feature importance ranking to identify the subset of features most relevant/predictive of the target variable and reduce dimensionality. However, after encoding a large dataset that mostly comprised categorical variables with numerous distinct values, our processed dataset exceeded the processing capacity of our Google Colab environment due to RAM limitations. To tackle this challenge, we utilized Orange Data Mining software to accomplish this necessary ranking process within our computational resource constraints. Features were ranked by gini decrease.
"""

#Top ranked by gini decrease

def feature_ranking(df):
  selected_columns = ['id','visible_in_reports', 'debit_code', 'use_client_address', 'type_code', 'class_id', 'is_registered', 'guarantee_gtor_type', 'last_maintenance_user', 'terminal_code', 'last_update_date_year', 'inception_date_month', 'risk_tolerance', 'inception_date_year', 'investment_objective', 'last_maintenance_time_year', 'label']
  df = df.drop(columns=df.columns.difference(selected_columns))
  return df

df = feature_ranking(df)

"""**Conditionally encoding categorical features**
> The following function applies label encoding to binary categorical features and one-hot-encoding to nominal categorical features.
"""

def encode_categorical(df):
    data_types = df.dtypes
    for column_name in df.columns:
        if data_types[column_name] == 'object':
            unique_count = df[column_name].nunique()
            if unique_count < 3: # binary
                label_encoder = LabelEncoder()
                df[column_name] = label_encoder.fit_transform(df[column_name])
            else: # nominal
                df = pd.get_dummies(df, columns=[column_name])
    return df

df = encode_categorical(df)

"""#Model Selection and Training

Two ensemble machine learning models, AdaBoost and GradientBoosting, were chosen for their suitability in tabular classification tasks and the dataset's characteristics - ensemble techniques, particularly boosting algorithms, were preferred due to their proficiency in handling intricate data structures, capturing non-linear relationships, and mitigating overfitting.  AdaBoost focuses on iteratively adjusting weights to prioritize misclassified samples, while GradientBoosting optimizes by fitting subsequent models to the residuals of the previous ones.

**Train Test Split**
"""

X = df.drop(columns=['label'])
y = df['label']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Hyperparameter Tuning**

> AdaBoost and GradientBoosting algorithms utilize decision trees as base estimators - thus, the number of trees plays a crucial role in the model's predictive performance. We explored testing 30, 50, and 70 trees to explore a range of values that balance between improving predictive performance and managing computational resources.

**AdaBoost Classifier**
"""

#hyperparameter tuning - n_estimators = 30
adaboost_clf_30 = AdaBoostClassifier(n_estimators= 30, random_state=1) # Initializing AdaBoost classifier
adaboost_clf_30.fit(X_train, y_train) # Training the AdaBoost classifier
y_pred = adaboost_clf_30.predict(X_test) # Inference on testing set
f1 = f1_score(y_test, y_pred, average='weighted') # Calculating F1 score

print("AdaBoost F1 Score for n_estimators = 30: ", f1)

#hyperparameter tuning - n_estimators = 50
adaboost_clf_50 = AdaBoostClassifier(n_estimators= 50, random_state=1)
adaboost_clf_50.fit(X_train, y_train)
y_pred = adaboost_clf_50.predict(X_test)
f1 = f1_score(y_test, y_pred, average='weighted')

print("AdaBoost F1 Score for n_estimators = 50: ", f1)

#hyperparameter tuning - n_estimators = 70
  adaboost_clf_70 = AdaBoostClassifier(n_estimators=70, random_state=1)
  adaboost_clf_70.fit(X_train, y_train)
  y_pred = adaboost_clf_70.predict(X_test) # Inference on testing set
  f1 = f1_score(y_test, y_pred, average='weighted') # Calculating F1 score

  print("AdaBoost F1 Score for n_estimators = 70: ", f1)

"""**GradientBoosting Classifier**"""

#hyperparameter tuning - n_estimators = 30
  gradientboost_clf_30 = GradientBoostingClassifier(n_estimators=30, random_state=1)
  gradientboost_clf_30.fit(X_train, y_train)
  y_pred = gradientboost_clf_30.predict(X_test)
  f1 = f1_score(y_test, y_pred, average='weighted')

  print("GradientBoosting F1 Score for n_estimators = 30: ", f1)

#hyperparameter tuning - n_estimators = 50
  gradientboost_clf_50 = GradientBoostingClassifier(n_estimators=50, random_state=1)
  gradientboost_clf_50.fit(X_train, y_train)
  y_pred = gradientboost_clf_50.predict(X_test)
  f1 = f1_score(y_test, y_pred, average='weighted')

  print("GradientBoosting F1 Score for n_estimators = 50: ", f1)

#hyperparameter tuning - n_estimators = 70
  gradientboost_clf_70 = GradientBoostingClassifier(n_estimators=70, random_state=1)
  gradientboost_clf_70.fit(X_train, y_train)
  y_pred = gradientboost_clf_70.predict(X_test)
  f1 = f1_score(y_test, y_pred, average='weighted')

  print("GradientBoosting F1 Score for n_estimators = 70: ", f1)

"""The GradientBoosting model, optimized with 70 estimators (trees), demonstrated the highest F1-score among the various configurations tested. As a result, this will be the model utilized for predicting customer churn using the provided test dataset.

**Retraining GradientBoosting Model on All Train Data**
"""

X = df.drop(columns=['label'])
y = df['label']  # Target variable

gradientboost_clf70 = GradientBoostingClassifier(n_estimators=70, random_state=1)
gradientboost_clf70.fit(X, y)
y_pred = gradientboost_clf70.predict(X)

"""###Inference on Test Data

**Loading Test Data**
"""

test_data_df = pd.read_csv("/content/drive/MyDrive/IIS Challenge/test.csv")

"""**Pre-processing Test Data**

Test data undergoes the same pre-proccesseing steps as the training data used for our model.
"""

test_data_df = missing_threshold(test_data_df, 30)
test_data_df = date_time_extraction(test_data_df)
test_data_df = impute_datetime(test_data_df)
test_data_df = feature_imputation(test_data_df)
test_data_df = feature_ranking(test_data_df)
test_data_df = encode_categorical(test_data_df)

"""Ensuring the Structure of the Test Dataset Aligns with the Train Dataset
> One-hot encoding generates new columns for every unique value in a feature. When the test set contains new distinct values not seen during training, this results in additional columns post-encoding. To ensure consistent model behavior, these extra columns must be removed before running inference.
"""

def align_datasets(train_df, test_df):

    test_extra_cols = set(test_df.columns) - set(train_df.columns)
    test_df.drop(columns=test_extra_cols, inplace=True) # Removing columns present in the encoded test dataset that were absent in the train dataset

    train_extra_cols = set(train_df.columns) - set(test_df.columns)
    for col in train_extra_cols:
        test_df[col] = 0 # Adding columns to the test dataset that are missing from the train dataset and defaulting htem to 0

    test_df = test_df[train_df.columns] # Making the column order of the datasets the same

    return test_df

test_data_df = align_datasets(df, test_data_df)

test_data_df.drop(columns=['label'], inplace=True)

"""**Running Inference on Test Data**"""

predictions = gradientboost_clf70.predict(test_data_df)  # Making predictions on processed test dataset
test_data_df['label'] = predictions
submission_df = test_data_df[['id', 'label']]  # Selecting only the 'id' and 'label' columns for submission
submission_df['label'] = submission_df['label'].replace({0: "Churn", 1: "No Churn"}) # Converting encoded labels back to original values
submission_df.to_csv('submission.csv', index=False)  # Saving submission file